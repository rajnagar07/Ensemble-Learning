{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMPmdWh5U8I-"
      },
      "outputs": [],
      "source": [
        "# Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "# Ans 1: Ensemble learning is a machine learning technique that combines multiple models to produce a single, more accurate predictive model.\n",
        "# The key idea is to aggregate the outputs of several individual models (often called \"weak learners\") to reduce errors like variance and bias,\n",
        "#  leading to a stronger and more robust final model than any single model could achieve alone.\n",
        "\n",
        "# Question 2: What is the difference between Bagging and Boosting?\n",
        "# Ans 2: Bagging\n",
        "# Process: Creates multiple models in parallel, each trained on a different random subset of the data (bootstrap samples).\n",
        "# Goal: Primarily reduces the variance of a model, which helps in preventing overfitting.\n",
        "# Model weighting: Each model is given equal importance during the final prediction.\n",
        "# Dependency: Models are independent and do not learn from each other.\n",
        "# Use case: Best for unstable models that have high variance and low bias.\n",
        "# Example: Random Forests is a well-known bagging algorithm.\n",
        "# Boosting\n",
        "# Process: Builds models sequentially, with each new model trying to correct the errors made by the previous ones.\n",
        "# Goal: Primarily reduces bias, and also variance, by focusing on difficult-to-classify examples.\n",
        "# Model weighting: Models are given different weights based on their performance, with more accurate models having a greater influence on the final prediction.\n",
        "# Dependency: Each new model is dependent on the results of the previous models.\n",
        "# Use case: Effective for both bias and variance errors, and is often used when a model has high bias.\n",
        "# Examples: AdaBoost and Gradient Boosting are common boosting algorithms.\n",
        "\n",
        "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "# Ans 3: Resampling with replacement: Bootstrap sampling creates new datasets by repeatedly drawing samples from the original dataset.\n",
        "# Each time a data point is selected, it is put back into the pool, meaning it can be selected multiple times in a single bootstrap sample.\n",
        "# Creates diverse datasets: Due to \"sampling with replacement,\" each bootstrap sample is different from the others. Some data points may appear\n",
        "# multiple times, while others may be omitted entirely.\n",
        "# Simulates multiple experiments: It gives the effect of having multiple, independent training sets without having to collect new data.\n",
        "# Role in Bagging and Random Forest\n",
        "# Generates diverse base models: In a Random Forest, each decision tree is trained on a different bootstrap sample. This ensures that each\n",
        "# tree learns from a slightly different perspective of the data.\n",
        "# Reduces overfitting: By training on different subsets, the models learn different patterns and nuances in the data, making the ensemble\n",
        "#  less likely to overfit to the noise of any single training set.\n",
        "# Combines predictions: After the models are trained, their predictions are combined to form a more robust and reliable final prediction.\n",
        "#  For classification, this is typically done through a \"majority vote,\" and for regression, it's done by averaging the predictions.\n",
        "\n",
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "# Ans 4: Out-of-Bag (OOB) samples are data points not used in training a specific model within an ensemble, which are then used to\n",
        "# calculate the OOB score.The OOB score provides an unbiased estimate of the model's performance on unseen data, similar to a\n",
        "# cross-validation score, without requiring a separate validation set\n",
        "\n",
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "# Ans 5: Decision Tree\n",
        "\n",
        "# Feature importance is calculated within a single tree.\n",
        "\n",
        "# Importance is based on decrease in impurity (e.g., Gini or Entropy) caused by each feature’s splits.\n",
        "\n",
        "# Highly sensitive to small changes in data — can change feature rankings easily.\n",
        "\n",
        "# May be biased toward features with many categories or continuous values.\n",
        "\n",
        "# Easier to interpret — you can clearly trace how each feature affects the output.\n",
        "\n",
        "# Can overfit to noise in the training data.\n",
        "\n",
        "# Provides a local view of feature importance (for that single model only).\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "# Feature importance is calculated across many decision trees in the forest.\n",
        "\n",
        "# Each tree’s feature importance is computed individually, then averaged over all trees.\n",
        "\n",
        "# More stable and reliable, since averaging reduces variance.\n",
        "\n",
        "# Less biased, as multiple trees reduce the effect of any single feature dominating by chance.\n",
        "\n",
        "# Harder to interpret because it’s an ensemble of many trees.\n",
        "\n",
        "# Reduces overfitting, giving better generalization performance.\n",
        "\n",
        "# Provides a global view of feature importance (overall dataset perspective)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPpDUpLLWSR4",
        "outputId": "507cdf1d-738f-4415-b437-041453efcbb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_model.fit(X_train, y_train)\n",
        "bag_pred = bag_model.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(f\"Single Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy:   {bag_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok0GnUdDWo6H",
        "outputId": "30179c58-ec7a-4482-9e3f-a6a75a766e24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0000\n",
            "Bagging Classifier Accuracy:   1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n",
        "print(\"\\nFinal Test Accuracy:\")\n",
        "print(f\"{final_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgn4OO0dXBLJ",
        "outputId": "a5a601d7-f283-4b9b-9e4b-d67e6ab5517a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'max_depth': None, 'n_estimators': 100}\n",
            "\n",
            "Final Test Accuracy:\n",
            "1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "bagging_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(f\"Bagging Regressor MSE:       {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeYH5uM0YN3r",
        "outputId": "571caaa5-91a6-4706-fca2-ecc72635398e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE:       0.2579\n",
            "Random Forest Regressor MSE: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "# default. You have access to customer demographic and transaction history data.\n",
        "# You decide to use ensemble techniques to increase model performance.\n",
        "# Explain your step-by-step approach to:\n",
        "# ● Choose between Bagging or Boosting\n",
        "# ● Handle overfitting\n",
        "# ● Select base models\n",
        "# ● Evaluate performance using cross-validation\n",
        "# ● Justify how ensemble learning improves decision-making in this real-world\n",
        "# context.\n",
        "\n",
        "# ans 10:\n",
        "# To predict loan default, first analyze data, remove leakage, and handle imbalance.\n",
        "# Choose Bagging (Random Forest) for stability or Boosting (XGBoost/LightGBM) for higher accuracy.\n",
        "# Prevent overfitting using regularization, early stopping, limited depth, and cross-validation.\n",
        "# Select base models like Decision Trees, Logistic Regression, or Gradient Boosted Trees.\n",
        "# Use Stratified or Time-based Cross-Validation and metrics like ROC-AUC, Precision, and Recall.\n",
        "# Evaluate calibration for reliable probabilities.\n",
        "# Ensemble learning improves decisions by combining multiple models, reducing bias and variance, increasing accuracy, and giving more reliable risk predictions—helping identify defaulters early and improving financial decision-making."
      ],
      "metadata": {
        "id": "yQ51g2l-Ygng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}